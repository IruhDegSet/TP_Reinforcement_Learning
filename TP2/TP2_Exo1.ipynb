{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from pprint import pprint\n",
    "from tqdm import tqdm \n",
    "from random import choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions= ['up', 'down', 'left', 'right']\n",
    "vectors= {'up': np.array([-1, 0]),\n",
    "          'right': np.array([0, 1]),\n",
    "          'down': np.array([1, 0]),\n",
    "          'left': np.array([0, -1])\n",
    "          }\n",
    "\n",
    "#Global variables\n",
    "num_rows= 3\n",
    "num_cols= 3\n",
    "num_actions= len(actions)\n",
    "initial_state= np.array([0, 0])\n",
    "final_state= np.array([2,2])\n",
    "gamma=0.7\n",
    "episode_length=20\n",
    "num_episodes=1000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **actions**: list of possible actions\n",
    "- **vectors**: dictionary for mapping each direction to its vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewards\n",
    "- **rewards**: the array of given rewards taken from the exo paper\n",
    "- **reward_function**: a dict for maping a actual state and the action taken by the agent to their respective reward {state: {direction: reward}}\\\n",
    "e.g.: \n",
    "```python\n",
    "{(0, 0): {'up': -1,\n",
    "          'down': 2/3}...\n",
    "          ...}\n",
    "```\n",
    "from state `(0, 0)` if the agent moves `up` it gets `-1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 0): {'down': 0.6666666666666666, 'left': -1.0, 'right': 1.0, 'up': -1.0},\n",
      " (0, 1): {'down': 2.0, 'left': 0.5, 'right': 1.5, 'up': -1.0},\n",
      " (0, 2): {'down': 2.5, 'left': 0.5, 'right': -1.0, 'up': -1.0},\n",
      " (1, 0): {'down': 1.5,\n",
      "          'left': -1.0,\n",
      "          'right': 1.3333333333333333,\n",
      "          'up': 0.3333333333333333},\n",
      " (1, 1): {'down': 3.0, 'left': 0.3333333333333333, 'right': 1.5, 'up': 0.25},\n",
      " (1, 2): {'down': 3.5, 'left': 1.0, 'right': -1.0, 'up': 0.25},\n",
      " (2, 0): {'down': -1.0, 'left': -1.0, 'right': 1.5, 'up': 0.5},\n",
      " (2, 1): {'down': -1.0, 'left': 1.0, 'right': 3.0, 'up': 0.8},\n",
      " (2, 2): {'down': -1.0, 'left': 0.8, 'right': -1.0, 'up': 0.5}}\n"
     ]
    }
   ],
   "source": [
    "rewards=np.array([\n",
    "    [[-1., 2/3, -1., 1.], [-1., 2., 1/2, 3/2], [-1., 5/2, 1/2, -1.]],\n",
    "    [[1/3, 3/2, -1., 4/3.], [1/4, 3., 1/3, 3/2], [1/4, 7/2, 1., -1.]],\n",
    "    [[1/2, -1., -1., 3/2], [4/5, -1., 1., 3.], [1/2, -1., 4/5, -1.]],\n",
    "    ])\n",
    "reward_function= {(i//num_rows, i%num_cols): {actions[j]: rewards[i//num_rows, i%num_cols, j] for j in range(num_actions)} for i in range(num_rows*num_cols)}\n",
    "pprint(reward_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policies:\n",
    "the policies follow a pattern where each policy is a dict mapping the state to the possible actions then we pick one randomly\\\n",
    "{state: possible actions} e.g:\n",
    "```python\n",
    "{(0, 0) : ['right'] ...}\n",
    "```\n",
    "if the agent is in state `(0, 0)` it will go `right`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy_1:\n",
      "{(0, 0): ['down'],\n",
      " (0, 1): ['down'],\n",
      " (0, 2): ['down'],\n",
      " (1, 0): ['down'],\n",
      " (1, 1): ['down'],\n",
      " (1, 2): ['down'],\n",
      " (2, 0): ['right'],\n",
      " (2, 1): ['right'],\n",
      " (2, 2): ['right']}\n",
      "\n",
      "policy_2:\n",
      "{(0, 0): ['right', 'down'],\n",
      " (0, 1): ['down'],\n",
      " (0, 2): ['down', 'left'],\n",
      " (1, 0): ['right'],\n",
      " (1, 1): ['up', 'down', 'left', 'right'],\n",
      " (1, 2): ['left'],\n",
      " (2, 0): ['up', 'right'],\n",
      " (2, 1): ['up'],\n",
      " (2, 2): ['up', 'left']}\n",
      "\n",
      "policy_3:\n",
      "{(0, 0): ['up', 'down', 'left', 'right'],\n",
      " (0, 1): ['up', 'down', 'left', 'right'],\n",
      " (0, 2): ['up', 'down', 'left', 'right'],\n",
      " (1, 0): ['up', 'down', 'left', 'right'],\n",
      " (1, 1): ['up', 'down', 'left', 'right'],\n",
      " (1, 2): ['up', 'down', 'left', 'right'],\n",
      " (2, 0): ['up', 'down', 'left', 'right'],\n",
      " (2, 1): ['up', 'down', 'left', 'right'],\n",
      " (2, 2): ['up', 'down', 'left', 'right']}\n"
     ]
    }
   ],
   "source": [
    "policy_1= {(i, j): ['down'] if i != 2 else ['right'] for i in range(num_rows) for j in range(num_cols)}\n",
    "policy_3= {(i, j): actions for i in range(num_rows) for j in range(num_cols)}\n",
    "policy_2= {\n",
    "    (0, 0): ['right', 'down'],\n",
    "    (0, 1): ['down'],\n",
    "    (0, 2): ['down', 'left'],\n",
    "    (1, 0): ['right'],\n",
    "    (1, 1): actions,\n",
    "    (1, 2): ['left'],\n",
    "    (2, 0): ['up', 'right'],\n",
    "    (2, 1): ['up'],\n",
    "    (2, 2): ['up', 'left']\n",
    "}\n",
    "print('policy_1:')\n",
    "pprint(policy_1)\n",
    "print('\\npolicy_2:')\n",
    "pprint(policy_2)\n",
    "print('\\npolicy_3:')\n",
    "pprint(policy_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Agent `Class`**: \n",
    "to instanciate an Agent with :\n",
    "- **attributs**:\n",
    "  - **policy**: how to act\n",
    "  - **initial_state**: from where to start\n",
    "  - **vectors**: how it can move\n",
    "  - **reward_function**: how much it gets\n",
    "- **methods**:\n",
    "  - **act(verbose)**: put the agent into work phase\n",
    "    - verbose: is for printing some logs about the agent\n",
    "    - returns: the reward and the action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "    def __init__(self, initial_state: np.ndarray, policy: dict, vectors: dict, reward_function: dict):\n",
    "        self.state= initial_state\n",
    "        self.policy= policy\n",
    "        self. vectors= vectors\n",
    "        self.reward_function= reward_function\n",
    "    \n",
    "    def act(self, verbose: int= 0):\n",
    "        action= choice(self.policy[tuple(self.state)]) # choosing randomly and uniformely from possible actions\n",
    "        reward= self.__get_reward(tuple(self.state), action)# get the associated reward for the action taken\n",
    "\n",
    "        if verbose:\n",
    "            log= f\"\"\"                   current state: {tuple(self.state)}\n",
    "                        choosen action: {action}\n",
    "                        reward: {reward}\n",
    "                \"\"\"\n",
    "            print(log)\n",
    "        \n",
    "        if reward == -1:\n",
    "            return reward, action\n",
    "        \n",
    "        # update agent's state\n",
    "        self.state= self.state + self.vectors[action]\n",
    "        if verbose:\n",
    "            print(f'new state= {self.state}')\n",
    "        return reward, action\n",
    "\n",
    "    def __get_reward(self, state: tuple, action: str):\n",
    "        reward= self.reward_function[state][action]\n",
    "        return reward\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate discount reward:\n",
    "is a function that returns the discount reward given a list of rewards [t, t+1, t+2, ..]\\\n",
    "disount reward<sub>t</sub>= gamma * R<sub>t+1</sub> + gamma<sup>2</sup> * R<sub>t+2</sub> + gamma<sup>3</sup> * R<sub>t+3</sub> ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_discount_reward(rewards, gamma= 0.5):\n",
    "    discount_reward = 0\n",
    "    for i, reward in enumerate(rewards):\n",
    "        discount_reward += reward * gamma**i\n",
    "    return discount_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo simulation \n",
    "is the function that returns the discount reawrd of the rewards of the episodes of simulation\n",
    "- if the agent arrives to destination (2, 2) it will stop from iterating \n",
    "- returns the discount reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_simulation(initial_state: tuple= initial_state, destination: tuple= final_state, policy: dict= policy_1, vectors: dict= vectors, reward_function: dict= reward_function, gamma: float= gamma, num_episodes: int= num_episodes, episode_length: int= episode_length, verbose: int= 0):\n",
    "    \n",
    "    agent= Agent(initial_state=initial_state, policy=policy, vectors=vectors, reward_function= reward_function)\n",
    "    rewards= []\n",
    "    agent_path= []\n",
    "    arrived= False\n",
    "\n",
    "    for i in tqdm(range(num_episodes)):\n",
    "        if np.array_equal(agent.state, np.array(destination)):\n",
    "                arrived= True\n",
    "                break\n",
    "        episode_rewards= []\n",
    "\n",
    "        for j in range(episode_length):\n",
    "            if np.array_equal(agent.state, np.array(destination)):\n",
    "                arrived= True\n",
    "                break\n",
    "            else:\n",
    "                reward, action= agent.act(verbose)\n",
    "                episode_rewards.append(reward)\n",
    "                agent_path.append(action)\n",
    "\n",
    "        \n",
    "        rewards.append(calculate_discount_reward(episode_rewards, gamma))\n",
    "\n",
    "    if arrived:   \n",
    "        print(f'arrived to destination in the episode {i} and step {j} ') \n",
    "    else:\n",
    "        print(\"didn't arrive\")\n",
    "\n",
    "    return calculate_discount_reward(rewards), agent_path\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to run the simulation\n",
    "def run_simulation(policies: list, initial_state: tuple= initial_state, destination: tuple= final_state, vectors: dict= vectors, reward_function: dict= reward_function, gamma: float= gamma, num_episodes: int= num_episodes, episode_length: int= episode_length, verbose: int= 0):\n",
    "    Vs= []\n",
    "    for policy in policies:\n",
    "        Vs.append(monte_carlo_simulation(initial_state= initial_state, destination= destination, policy= policy, vectors= vectors, reward_function= reward_function, gamma= gamma, num_episodes= num_episodes, episode_length= episode_length, verbose= verbose)[0])\n",
    "    return Vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1000 [00:00<00:04, 223.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arrived to destination in the episode 1 and step 4 \n",
      "\n",
      "the discount reward is: 3.480666666666666\n",
      "the agent's path:\n",
      "down --> down --> right --> right\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dr, path= monte_carlo_simulation(policy=policy_1)\n",
    "print()\n",
    "print(f'the discount reward is: {dr}')\n",
    "print('the agent\\'s path:')\n",
    "print(' --> '.join(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1000 [00:00<00:00, 1840.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arrived to destination in the episode 1 and step 4 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 2456.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "didn't arrive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/1000 [00:00<00:00, 1090.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arrived to destination in the episode 2 and step 17 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.480666666666666, 8.388361071424578, -0.26654527489276497]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vs= run_simulation(policies= [policy_1, policy_2, policy_3])\n",
    "Vs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
